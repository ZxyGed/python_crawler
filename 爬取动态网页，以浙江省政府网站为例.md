爬取动态网页，以浙江省政府网站为例

首先点击F12查看网页源码，点击 "网络->XHR"，点击底部的页面导航栏，进行页面跳转，可以看到XHR接收到一个POST信息，查看该信息的标头，其请求的网站为真实的网站地址，复制其请求标头的内容。

![1569720954699](C:\Users\15487\AppData\Roaming\Typora\typora-user-images\1569720954699.png)

查看其正文字段，相应正文中正是我们想要爬取的内容

![1569721104661](C:\Users\15487\AppData\Roaming\Typora\typora-user-images\1569721104661.png)

查看其请求正文，通过点击底部的页面跳转栏不断跳转并比较报文信息，我们可以发现currpage即请求的页面，而infotypeld是请求的模块，即"政府文件"、"政府规章"、"地方法规"之类的

![1569721164616](C:\Users\15487\AppData\Roaming\Typora\typora-user-images\1569721164616.png)

因此可以进入爬取的代码阶段了，我们需要请求真实的网址，同时复制一样的请求报文，配上不同的模块号和页面号，之后只需要遍历网页，然后用pyquery解析想要的字段即可。

TIPS: 

- 在标头的请求标头栏，可以按住 CTRL，同时复制多个字段，然后在 jupyter 中粘贴后在每行的末尾配上 `;\` 然后用split分割后构造成header 
- pyquery需要调用 `print()` 才能查看爬取的源码
- pyquery最重要的方法是 `eq(i)` 用来获取筛选出来的第 i 个元素

```python
# 爬取政府文件模块的所有信息
import sys
sys.path.append(
    r"c:\users\15487\appdata\local\continuum\anaconda3\lib\site-packages")
import pandas as pd
from pyquery import PyQuery as pq

# 政府文件的真实请求url
durl='http://www.zj.gov.cn/module/xxgk/search.jsp?texttype=0&fbtime=&vc_all=&vc_filenumber=&vc_title=&vc_number=&currpage=6&sortfield=,compaltedate:0'
Cookie='zh_choose_undefined=s; gs_b_tas=LPXKNM2L17685EYH; acw_tc=76b20fef15696711718028647e388fbcc47431353ac9fa891efe92c63b7a23; SERVERID=d2e5436f826f0ca88944db105fd8663e|1569677655|1569675417'
header={
    'Accept':'*/*',
    'Accept-Encoding':'gzip,deflate',
    'Accept-Language':'zh-Hans-CN, zh-Hans; q=0.5',
    'Cache-Control': 'no-cache',
    'Connection': 'Keep-Alive',
    'Content-Length': '258',
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    'Cookie':Cookie,
    'Host': 'www.zj.gov.cn',
    'Origin': 'http://www.zj.gov.cn',
    'Referer': 'http://www.zj.gov.cn/col/col1545735/index.html',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763',
    'X-Requested-With': 'XMLHttpRequest'
}
mcs=[]
whs=[]
hrefs=[]
jgs=[]
rqs=[]
syhs=[]
currpage=1
for currpage in range(1,119):
    data={
       'area': '000014349',
       'currpage': currpage,
       'divid': 'div1545735',
       'infotypeId': 'B0502',
       'jdid': '3096', 
       'sortfield':',compaltedate:0',
       'texttype': '0'
    }
    ddoc=pq(durl,data=data,header=header)
    dtarget=ddoc('td a').filter('.bt_link')
    #wh发文字号、href文件链接、jg发文部门、rq公开日期、syh索引号
    length=len(dtarget)
    for i in range(length):
        mcs.append(dtarget.eq(i).attr('mc'))
        whs.append(dtarget.eq(i).attr('wh'))
        hrefs.append(dtarget.eq(i).attr('href'))
        jgs.append(dtarget.eq(i).attr('jg'))
        rqs.append(dtarget.eq(i).attr('rq'))
        syhs.append(dtarget.eq(i).attr('syh'))

yxs=[] #有效性
# 一个一个网页点开，有的网页有文件有效性，有的没有，需要判断其前一个节点的内容是"有 效 性："
for href in hrefs:
    if(pq(href,encoding='utf-8')('tbody tr').eq(2)('td').eq(2).text()=='有 效 性：'):
        yxs.append(pq(hrefs[3],encoding='utf-8')('tbody tr').eq(2)('td').eq(3).text())
    else:
        yxs.append("None")

s=['浙江省' for i in range(len(mcs))] #省/市填充字段

done=pd.DataFrame({'省/市':s,'索引号':syhs,'标题':mcs,'发文字号':whs,'发文部门':jgs,'公开日期':rqs,'有效性':yxs,'文件链接':hrefs})
done.to_csv(r'C:\Users\15487\Desktop\政府文件.csv',index=None,sep=',',encoding='gb2312')
```

